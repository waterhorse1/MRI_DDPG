workers: 4
display: 20
batch_size: 12
#start_iters: 0
num_episodes: 20002 #30000
test_episodes: 500 #3000
save_episodes: 4000
iter_size: 2
resume_model: '' #'model/8_28_21_16000.pth'
#-------------rl_related--------------------#
v_loss_coeff: 1
switch: 10
warm_up_episodes: 1000
episode_len: 3 #2
gamma: 1 #0.5 #0.95
beta: 0.1
num_actions: 13
move_range: 3
reward_method: 'abs'
#reward_method: 'ssim'
#-------------lr_policy--------------------#
base_lr: 0.01
# step
#lr_policy: 'step'
#policy_parameter:
#  gamma: 0.5
#  step_size: 2000
# exp
# lr_policy: 'exp'
# policy_parameter:
#   gamma: 0.99
# 
# inv
# lr_policy: 'inv'
# policy_parameter:
#   gamma: 0.1
#   power: 0.1
# 
# multistep
#lr_policy: 'multistep'
#policy_parameter:
  #stepvalue: [20000, 35000, 45000]
  #gamma: 0.1
# 
# poly
lr_policy: 'poly'
policy_parameter:
  power: 0.9
  max_iter: 40000
# 
# sigmoid
# lr_policy: 'sigmoid'
# policy_parameter:
#   gamma: 0.9
#   stepsize: 5000
#lr_policy: 'multistep-poly'
#policy_parameter:
  #stepvalue: [12435, 24870, 37350, 49740]
  #max_iter: 62175
  #gamma: 0.333
  #power: 1.2
